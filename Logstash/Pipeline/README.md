

## ğŸ’¡ Why Do We Need a Pipeline in Logstash?

A **pipeline** in Logstash is **the brain that handles your logs** â€” from how they come in, how theyâ€™re processed, and where they go.

---

## ğŸ“¦ Think of a Pipeline Like This:

> "Raw logs come in â†’ Logstash cleans and structures them â†’ Sends them to Elasticsearch"

Without a pipeline, Logstash wouldnâ€™t know:
- **Where to receive the logs**
- **How to parse or filter the logs**
- **Where to send the logs**

---

## ğŸ§± Pipeline = 3 Building Blocks

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT  â”‚ ==> â”‚ FILTER  â”‚ ==> â”‚  OUTPUT     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Letâ€™s break down each:

| Part    | What it Does                            | Example                        |
|---------|------------------------------------------|--------------------------------|
| `input` | Where the logs are coming from           | Filebeat, syslog, Kafka        |
| `filter`| How logs are parsed and enriched         | grok, date, geoip, mutate      |
| `output`| Where to send the logs                   | Elasticsearch, file, stdout    |

---

## âœ… What Pipelines Let You Do:

- ğŸ” **Parse raw logs** (turn plain text into structured data)
- ğŸ“… **Fix timestamps**
- ğŸ” **Anonymize or enrich data** (e.g., add geo-location from IP)
- ğŸ“¤ **Send to multiple places** (e.g., Elasticsearch + backup file)

---

## ğŸ“ˆ Without Pipelines?

If you didnâ€™t have a pipeline:
- Logstash wouldn't accept any logs
- Nothing would get parsed or structured
- You wouldnâ€™t see anything in Kibana

---

## ğŸ” Real-Life Example (Simple Flow)

```
[ Filebeat ]  =>  [ Logstash Pipeline ]
                      â”œâ”€ input (beats)
                      â”œâ”€ filter (grok, date, mutate)
                      â””â”€ output (Elasticsearch)
                                â†“
                          [ Kibana Dashboard ]
```

> ğŸ”¹ Getting logs from Filebeat  
> ğŸ”¹ Sending them to Logstash  
> ğŸ”¹ Parsing them in Logstash  
> ğŸ”¹ Then storing them in Elasticsearch  
> ğŸ”¹ Finally, visualizing them in Kibana

---

## ğŸ“„ Text Architecture: ELK Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Filebeat  â”‚ â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  Logstash  â”‚ â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ Elasticsearch â”‚ â”€â”€â”€â”€â”€â–¶  â”‚   Kibana     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚       Logstash Pipeline Structure       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚              â”‚              â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  input  â”‚ => â”‚  filter   â”‚ =>â”‚   output     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚              â”‚              â”‚
       â–¼              â–¼              â–¼
  beats plugin     grok, date,     elasticsearch
                   mutate, etc.    output plugin
```

---

## âœ… Step-by-Step Example

### ğŸ¯ 1. Filebeat (Log Forwarder)
- Installed on the client/server sending logs
- Sends logs to Logstash on port `5044`

**Filebeat config:**
```yaml
output.logstash:
  hosts: ["logstash-server-ip:5044"]
```

---

### ğŸ¯ 2. Logstash Pipeline Configuration (`/etc/logstash/conf.d/app-logs.conf`)

```ruby
input {
  beats {
    port => 5044
  }
}

filter {
  grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
  }
  date {
    match => ["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]
    target => "@timestamp"
  }
}

output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "app-logs-%{+YYYY.MM.dd}"
  }
}
```

---

## ğŸ” How the Pipeline Works

| Component       | What It Does                                                                 |
|-----------------|------------------------------------------------------------------------------|
| **input**       | Accepts logs from Filebeat (via Beats input plugin)                          |
| **filter**      | Parses logs using `grok`, sets proper timestamp using `date` plugin          |
| **output**      | Sends structured logs to Elasticsearch using the `elasticsearch` plugin      |

---

## ğŸ“‚ Pipeline File Location

Put the config file in:
```
/etc/logstash/conf.d/app-logs.conf
```

Then restart Logstash:
```bash
sudo systemctl restart logstash
```

